---
layout: article
title: Decision Trees and Ensembling
description: Also covers Bootstrap & Bagging, Boosting, Random Forests
imgsrc: assets/pics/cs3244/9.png
banner: assets/pics/cs3244/ml_banner.jpg
date: 2017-11-20
category: CS3244
tags: [Machine Learning]
author: Song Jiaming
---

This article is a summary note for CS3244 week 9 content. The note also includes some contents from Machine Learning Technique Course of National University of Taiwan.

## 1. Decision Trees
People often focus on just a few questions (dimensions) of an event.

Imperative programming control flow also embodies this. (if sth do sth, else do sth)
- For example:
![Tennis]({{site.baseurl}}/assets/pics/cs3244/chap9/tennis.png)

__Parts of a tree__:
1. Internal nodes, aka tests, a node tests exactly 1 component of x
2. A branch in the tree corresponds to a test result
    - An attribute value or a range of attribute values
3. Each leaf node assigns:
    - A class y ∈ {discrete classes} : (classification) decision tree
    - Real value y ∈ R: regression tree

__Decision Boundary__:
![Tennis2]({{site.baseurl}}/assets/pics/cs3244/chap9/tennis2.png)

__Analyzing Decision Trees__:

How expressive is the H of decision trees?
- It can express/shatter any point (Each x can be in a class y). ⇒ $d_{vc} = \infty$
- If no noise in y, one can always have $E_{in} = 0$
- However, it means that the trees have overfitting, high variance
    - How to counter this problem? Make smaller trees (i.e. lower bias) 
    
__Important decisions are prefered to be at the top.__ 3 questions derived from above:
1. How do we pick a feature(branching critera) to split on?
2. How do we discretize continuous features?
3. How do we decide where to prune? (prune back complete trees) (when to stop growing the tree, doesn't work well in practice)


### Decision Tree Learning
__Path view__<br>
- view the tree in the perspective of leafs, each leaf is in a path (from root to leaf)
    - $G(x) = \sum\limits_{t=1}^{T}\, [[\text{is x on path t ?}]]\cdot g_{t}(x)$
    - G(x): full-tree hypothesis
    - $g_{t}(x)$: base hypothesis, leaf at the end of path t, a constant

__Recursive view__<br>
- view from the root, separate the tree into a few branches
    + $G(X) = \sum\limits_{c=1}^{C}\, [[b(x) = c?]]\cdot G_{c}(x)$
    + b(x): branching criteria
    + $G_{c}(x0$: sub-tree hypothesis at the c-th branch
- Idea:  (Recursively) choose most significant attribute as root
of (sub) tree

__A Basic Decision Tree Algorthm__
- Given dataset D = ${(x_{1},y_{1}),...,(x_{N},y_{N})}$
```
function DTL(D):
    if termination critera is met:
        return base hypothesis g_t(x)
    else:
        1) learn branching criteria b(x)
        2) split D to C parts: Dc = {(xn,yn): b(xn) = c}
        3) build subtree Gc = DLT(Dc) #build subtree based on branches
        4) return G(x)
```

- More detailed pseudocode:
    - attributes: features
    - default: base hypothesis $g_{t}(x)$
    - CHOOSE_ATTRIBUTE: choose the best attribute out of a set
```python
def DTL(D, attributes, default):
    if (D is empty): return default  # no example
    elif (all examples have the same classification): return the classification
    elif (attributes is empty): return MODE(D) # majority classification
    else:
        best = CHOOSE_ATTRIBUTE(attributes, D)
        tree = A new decision tree with best as the root
        for i in len(best):
            v = best[i]
            D[i] = {elements in examples with best = v}
            subtree = DTL(D[i], attributes= best, MODE(D))
            Add a branch to tree with label = v and subtree = subtree
        return tree
```


__Choosing an attribute__
- Idea: a good attribute splits the training set into subsets that are "all positive" or "all negative" (purity)
![Attribute]({{site.baseurl}}/assets/pics/cs3244/chap9/attribute.png)
- Patrons is a better choice, because of its purity of the subproblems.


Now, let's answer the 3 questions.
### 1.1 How do we pick a feature to split on?

__Classification and Regression Tree (C&RT) way__
- 2 simple choices:
    + C = 2 (binary tree)
    + $g_{t}(x)$ is the constant that $\min E_{in}$
        * For Binary/Multiclass classification (0/1 errors): it is the majority of ${y_{n}}$
        * For Regression (squared error): it is the average of ${y_{n}}$
- More simple choices:
    + simple internal node for C = 2:{1,2}-output decision stump
    + 'easier' sub-tree: branch by purifying
        * $b(x) = \underset{\text{decision stumps h(x)}}{\text{argmin}}\,\,\sum\limits_{c=1}^{2} \|D_{c} \text{with h}\| \cdot \text{impurity}(D_{c} \text{with h})$
    - i.e. Bi-branching by purifying

<br>
To implement CHOOSE_ATTRIBUTE in DTL, we first need a concept to __measure purity__.<br>
__By $E_{in}$ of optimal constant__
+ Since in the final step, we need to return a constant $g_{t}(x)$, if this constant is good, we say the purity is good
+ Regression error: __impurity(D) =__ $\frac{1}{N}\sum\limits_{n=1}^{N}(y_{n}-\bar{y})^{2}$ with $\bar{y}$ = average of all y
+ Classification error:  __impurity(D) =__ $\frac{1}{N}\sum\limits_{n=1}^{N}[[y_{n}\not = y^{\ast}]]$ with $y^{\ast}$ = majority of y
    * popular choices - Gini index: __impurity(D) =__ $1-\sum\limits_{k=1}^{K}(\frac{\sum\limits_{n=1}^{N}[[y_{n} = k]]}{N})$
- Termination in C&RT: forced to terminate when:
    + all $y_{n}$ the same: impurity = 0 $\implies g_{t}(x) = y_{n}$
    + all $x_{n}$ the same: no decision stumps (fully-grown tree)

__Entropy__
- $H(X) = - \sum\limits_{i=0}^{C} \, p\, \log_{2}(p)$
    + X is the whole dataset
    + C is the number of possible classifications of points in X. e.g. if a bag of marbles contains red, blue, green marbles, then C is 3.
- e.g. For a training set containing __p__ positive examples and __n__ negative examples, its entropy is:
    + $H(\frac{p}{p+n},\frac{n}{p+n}) = -\frac{p}{p+n}\log(\frac{p}{p+n}) -\frac{n}{p+n}\log\frac{n}{p+n}$


|Entropy |  Curve |
|:-------------------------:|:-------------------------:|
|  For $\frac{p}{p+n}$, the 2-class entropy is:<br> 0 when $\frac{p}{p+n} = 0$<br>1 when $\frac{p}{p+n} = 0.5$<br>0 when $\frac{p}{p+n} = 1$ <br> monotonically increasing between 0 and 0.5<br> monotonically decreasing between 0.5 and 1 |  ![Entropy]({{site.baseurl}}/assets/pics/cs3244/chap9/entropy.png)|

__Information gain__
- A chosen feature $x_{i}$ divides the example set S into subsets $S_{1},...S_{C}$ according to the $C_{i}$ values of $x_{i}$
- The entropy of S then reduces to the entropy of each $S_{1},...S_{C}$:
    + $\text{remainder}(S,x_{i})$ = $\sum\limits_{j=1}^{C_{i}} \, \frac{\|S_{j}\|}{\|S\|}\, H(S_{j})$
- There is Information Gain (IG; “reduction in entropy”) from knowing the value
of $x_{i}$.
- Choose the attribute with the largest IG:
    - __IG(S,$x_{i}$) = H(S) - remainder(S,$x_{i}$)__
- e.g. For the Patron graph above,
    + The training set at the root: p=n=6, $H(\frac{6}{12},\frac{6}{12})=1$
    + Consider the attributes Patrons and Type:
        * IG(Patrons) = $1 - [ \frac{2}{12} H(0,1)+\frac{4}{12} H(1,0)+\frac{6}{12}H(\frac{2}{6},\frac{4}{6})] = 0.0541$ bits
        * IG(Type) = $1 - [ \frac{2}{12} H(\frac{1}{2},\frac{1}{2}))+ \frac{2}{12} H(\frac{1}{2},\frac{1}{2}))+\frac{4}{12} H(\frac{2}{4},\frac{2}{4}))++\frac{4}{12} H(\frac{2}{4},\frac{2}{4}))]=0$ bits
    + Patrons has the highest IG and it will be chosen by DTL as the root 

### 1.2 How do we discretize continuous features?
How can we choose among an infinite number of split points for a continuous feature $x_{j}$?

![IG]({{site.baseurl}}/assets/pics/cs3244/chap9/ig.png)

### 1.3. How do we decide where to stop pruning?
Use validation! Remove node and replace by MODE of labels y, if pruned tree performs better than original over validation set













<br>

----
#### References
1. [Decision Tree Hypothesis @ Machine Learning Techniques](https://youtu.be/dAqPpAXnMJ4)
2.  [Decision Tree Algorithm @ Machine Learning Techniques](https://youtu.be/s9Um2O7N7YM)
3.  [Decision Tree Heuristics in C&RT @ Machine Learning Techniques](https://youtu.be/uvGC_Y0EYiA)
4.  [ Decision Tree in Action @ Machine Learning Techniques](https://youtu.be/ryWTrPPbqcg)